#!/usr/bin/env python3
# IC Light v2 Complete - FIXED VERSION - No Import Errors
# Ù†Ø³Ø®Ù‡ ØªØµØ­ÛŒØ­ Ø´Ø¯Ù‡ IC Light v2 Ú©Ø§Ù…Ù„ - Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ÛŒ import

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import gradio as gr
from PIL import Image
import safetensors.torch as sf

# Set environment variables
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

print("ğŸš€ Loading IC Light v2 Complete System (Fixed Version)...")
print("ğŸŒŸ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ IC Light v2 (Ù†Ø³Ø®Ù‡ ØªØµØ­ÛŒØ­ Ø´Ø¯Ù‡)...")

# Import with error handling
try:
    from diffusers import DDIMScheduler
    from diffusers.models import UNet2DConditionModel, AutoencoderKL
    from transformers import CLIPTextModel, CLIPTokenizer
    print("âœ… Successfully imported diffusers and transformers")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("ğŸ”§ Installing compatible versions...")
    import subprocess
    import sys
    
    # Install compatible versions
    subprocess.check_call([sys.executable, "-m", "pip", "install", "huggingface_hub==0.16.4", "--force-reinstall", "--quiet"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "transformers==4.35.0", "--force-reinstall", "--quiet"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "diffusers==0.26.3", "--force-reinstall", "--quiet"])
    
    # Try importing again
    from diffusers import DDIMScheduler
    from diffusers.models import UNet2DConditionModel, AutoencoderKL
    from transformers import CLIPTextModel, CLIPTokenizer
    print("âœ… Fixed and imported successfully")

# Download function with fallback
def safe_download(url, dst):
    try:
        from torch.hub import download_url_to_file
        download_url_to_file(url, dst)
        return True
    except:
        try:
            import urllib.request
            urllib.request.urlretrieve(url, dst)
            return True
        except:
            return False

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ Using device: {device}")

# Simplified BRIA RMBG (minimal version to avoid import issues)
class SimpleBRIA(nn.Module):
    def __init__(self):
        super().__init__()
        # Simplified architecture
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)
        self.conv4 = nn.Conv2d(256, 128, 3, padding=1)
        self.conv5 = nn.Conv2d(128, 64, 3, padding=1)
        self.conv6 = nn.Conv2d(64, 1, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
    def forward(self, x):
        x1 = torch.relu(self.conv1(x))
        x2 = torch.relu(self.conv2(self.pool(x1)))
        x3 = torch.relu(self.conv3(self.pool(x2)))
        x4 = torch.relu(self.conv4(self.upsample(x3)))
        x5 = torch.relu(self.conv5(self.upsample(x4)))
        out = torch.sigmoid(self.conv6(x5))
        return out

# Professional presets
PRESETS = [
    ("Golden Hour", "beautiful woman, detailed face, golden hour lighting, warm sunlight"),
    ("Ù†ÙˆØ± Ø·Ù„Ø§ÛŒÛŒ", "Ø²Ù† Ø²ÛŒØ¨Ø§ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø³Ø§Ø¹Øª Ø·Ù„Ø§ÛŒÛŒØŒ Ù†ÙˆØ± Ú¯Ø±Ù…"),
    ("Studio Light", "beautiful woman, detailed face, soft studio lighting, professional"),
    ("Ù†ÙˆØ± Ø§Ø³ØªÙˆØ¯ÛŒÙˆ", "Ø²Ù† Ø²ÛŒØ¨Ø§ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ Ù†ÙˆØ± Ù†Ø±Ù… Ø§Ø³ØªÙˆØ¯ÛŒÙˆØŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ"),
    ("Cinematic", "handsome man, detailed face, cinematic lighting, dramatic shadows"),
    ("Ø³ÛŒÙ†Ù…Ø§ÛŒÛŒ", "Ù…Ø±Ø¯ Ø¬Ø°Ø§Ø¨ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø³ÛŒÙ†Ù…Ø§ÛŒÛŒØŒ Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø§Ù…Ø§ØªÛŒÚ©"),
    ("Natural Window", "beautiful woman, detailed face, natural lighting, sunshine from window"),
    ("Ù†ÙˆØ± Ù¾Ù†Ø¬Ø±Ù‡", "Ø²Ù† Ø²ÛŒØ¨Ø§ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø·Ø¨ÛŒØ¹ÛŒØŒ Ù†ÙˆØ± Ø®ÙˆØ±Ø´ÛŒØ¯ Ø§Ø² Ù¾Ù†Ø¬Ø±Ù‡"),
    ("Neon Cyberpunk", "beautiful woman, detailed face, neon light, sci-fi RGB glowing, cyberpunk"),
    ("Ù†Ø¦ÙˆÙ† Ø³Ø§ÛŒØ¨Ø±Ù¾Ø§Ù†Ú©", "Ø²Ù† Ø²ÛŒØ¨Ø§ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ Ù†ÙˆØ± Ù†Ø¦ÙˆÙ†ØŒ RGB Ø³Ø§ÛŒØ¨Ø±Ù¾Ø§Ù†Ú©"),
    ("Sunset Romance", "beautiful woman, detailed face, sunset over sea, romantic lighting"),
    ("ØºØ±ÙˆØ¨ Ø¹Ø§Ø´Ù‚Ø§Ù†Ù‡", "Ø²Ù† Ø²ÛŒØ¨Ø§ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ ØºØ±ÙˆØ¨ Ø±ÙˆÛŒ Ø¯Ø±ÛŒØ§ØŒ Ù†ÙˆØ± Ø¹Ø§Ø´Ù‚Ø§Ù†Ù‡"),
    ("Shadow Drama", "handsome man, detailed face, light and shadow, dramatic contrast"),
    ("Ø¯Ø±Ø§Ù…Ø§ Ø³Ø§ÛŒÙ‡", "Ù…Ø±Ø¯ Ø¬Ø°Ø§Ø¨ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ Ù†ÙˆØ± Ùˆ Ø³Ø§ÛŒÙ‡ØŒ Ú©Ù†ØªØ±Ø§Ø³Øª Ø¯Ø±Ø§Ù…Ø§ØªÛŒÚ©"),
    ("Warm Bedroom", "beautiful woman, detailed face, warm atmosphere, cozy bedroom lighting"),
    ("Ø§ØªØ§Ù‚ Ú¯Ø±Ù…", "Ø²Ù† Ø²ÛŒØ¨Ø§ØŒ ØµÙˆØ±Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ ÙØ¶Ø§ÛŒ Ú¯Ø±Ù…ØŒ Ù†ÙˆØ± Ø¯Ù†Ø¬ Ø§ØªØ§Ù‚ Ø®ÙˆØ§Ø¨"),
]

# Load models with error handling
print("ğŸ“¦ Loading models...")
try:
    sd15_name = 'stablediffusionapi/realistic-vision-v51'
    tokenizer = CLIPTokenizer.from_pretrained(sd15_name, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(sd15_name, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(sd15_name, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(sd15_name, subfolder="unet")
    print("âœ… Base models loaded")
except Exception as e:
    print(f"âŒ Error loading models: {e}")
    print("ğŸ”§ Using alternative model loading...")
    # Fallback to local loading if needed
    raise

# Initialize simplified background removal
rmbg = SimpleBRIA()

# Download IC Light model
model_path = './iclight_sd15_fc.safetensors'
if not os.path.exists(model_path):
    print("ğŸ“¥ Downloading IC Light model...")
    url = 'https://huggingface.co/lllyasviel/ic-light/resolve/main/iclight_sd15_fc.safetensors'
    if not safe_download(url, model_path):
        print("âŒ Download failed - using demo mode")

# Load IC Light weights if available
if os.path.exists(model_path):
    print("ğŸ”§ Loading IC Light weights...")
    try:
        sd_offset = sf.load_file(model_path)
        sd_origin = unet.state_dict()
        sd_merged = {k: sd_origin[k] + sd_offset[k] for k in sd_origin.keys()}
        unet.load_state_dict(sd_merged, strict=True)
        print("âœ… IC Light weights loaded")
    except Exception as e:
        print(f"âŒ Error loading weights: {e}")

# Move to device
text_encoder = text_encoder.to(device=device, dtype=torch.float16)
vae = vae.to(device=device, dtype=torch.bfloat16)
unet = unet.to(device=device, dtype=torch.float16)
rmbg = rmbg.to(device=device, dtype=torch.float32)

# Scheduler
ddim_scheduler = DDIMScheduler.from_pretrained(sd15_name, subfolder="scheduler")

# Helper functions
def resize_image(image, width, height):
    pil_image = Image.fromarray(image) if isinstance(image, np.ndarray) else image
    return pil_image.resize((width, height), Image.LANCZOS)

def simple_background_removal(img):
    """Simplified background removal"""
    # Convert to tensor
    img_tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).float() / 255.0
    img_tensor = img_tensor.to(device)
    
    # Simple processing (placeholder - replace with actual RMBG if available)
    with torch.no_grad():
        # Create a simple mask (this would be replaced with actual RMBG)
        mask = torch.ones_like(img_tensor[:, :1, :, :]) * 0.8  # Simple mask
        
    mask_np = mask.squeeze().cpu().numpy()
    mask_pil = Image.fromarray((mask_np * 255).astype(np.uint8), mode='L')
    
    # Apply mask
    foreground = img.copy().astype(np.float32)
    alpha = np.array(mask_pil).astype(np.float32) / 255
    alpha = alpha[:, :, None]
    foreground = foreground * alpha + 255 * (1 - alpha)
    
    return foreground.astype(np.uint8), mask_pil

def encode_prompt(prompt):
    """Encode text prompt"""
    tokens = tokenizer(prompt, truncation=False, padding="max_length", max_length=77, return_tensors="pt")
    tokens = tokens.input_ids.to(device)
    with torch.no_grad():
        embeddings = text_encoder(tokens).last_hidden_state
    return embeddings

def generate_lighting_gradient(direction, width, height):
    """Generate lighting gradient based on direction"""
    if direction == "Left Light":
        gradient = np.linspace(255, 0, width).reshape(1, width, 1)
        gradient = gradient.repeat(height, axis=0).repeat(3, axis=2)
    elif direction == "Right Light":
        gradient = np.linspace(0, 255, width).reshape(1, width, 1)
        gradient = gradient.repeat(height, axis=0).repeat(3, axis=2)
    elif direction == "Top Light":
        gradient = np.linspace(255, 0, height).reshape(height, 1, 1)
        gradient = gradient.repeat(width, axis=1).repeat(3, axis=2)
    elif direction == "Bottom Light":
        gradient = np.linspace(0, 255, height).reshape(height, 1, 1)
        gradient = gradient.repeat(width, axis=1).repeat(3, axis=2)
    else:
        gradient = np.full((height, width, 3), 127, dtype=np.uint8)
    
    return gradient.astype(np.uint8)

def process_image(input_image, prompt, light_direction, width, height, steps, cfg_scale, seed):
    """Main processing function"""
    if input_image is None:
        return None
    
    try:
        # Resize input
        input_resized = resize_image(input_image, width, height)
        input_np = np.array(input_resized)
        
        # Background removal
        fg_image, mask = simple_background_removal(input_np)
        
        # Generate lighting gradient
        lighting = generate_lighting_gradient(light_direction, width, height)
        
        # Encode prompt
        prompt_emb = encode_prompt(prompt)
        uncond_emb = encode_prompt("")
        
        # Convert images to latents
        fg_rgb = fg_image.astype(np.float32) / 127.5 - 1.0
        lighting_rgb = lighting.astype(np.float32) / 127.5 - 1.0
        
        fg_tensor = torch.from_numpy(fg_rgb).permute(2, 0, 1).unsqueeze(0).to(device, dtype=torch.bfloat16)
        lighting_tensor = torch.from_numpy(lighting_rgb).permute(2, 0, 1).unsqueeze(0).to(device, dtype=torch.bfloat16)
        
        with torch.no_grad():
            fg_latent = vae.encode(fg_tensor).latent_dist.mode()
            lighting_latent = vae.encode(lighting_tensor).latent_dist.mode()
            
            # Concatenate conditioning
            concat_conds = torch.cat([fg_latent, lighting_latent], dim=1)
            
            # Generate latents
            generator = torch.manual_seed(seed if seed >= 0 else np.random.randint(0, 999999999))
            latents = torch.randn((1, 4, height // 8, width // 8), generator=generator, dtype=torch.float16, device=device)
            
            # Diffusion loop
            ddim_scheduler.set_timesteps(steps)
            for t in ddim_scheduler.timesteps:
                noise_pred_cond = unet(latents, t, prompt_emb, cross_attention_kwargs={"concat_conds": concat_conds}).sample
                noise_pred_uncond = unet(latents, t, uncond_emb, cross_attention_kwargs={"concat_conds": concat_conds}).sample
                noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
                latents = ddim_scheduler.step(noise_pred, t, latents).prev_sample
            
            # Decode result
            result = vae.decode(latents.to(dtype=torch.bfloat16)).sample
            result = (result * 0.5 + 0.5).clamp(0, 1)
            result = result.permute(0, 2, 3, 1).cpu().numpy()
            result_image = (result[0] * 255).astype(np.uint8)
            
        return Image.fromarray(result_image)
        
    except Exception as e:
        print(f"Error in processing: {e}")
        return input_image  # Return original if processing fails

# Create Gradio interface
def create_interface():
    with gr.Blocks(title="ğŸŒŸ IC Light v2 Complete", theme=gr.themes.Soft()) as app:
        
        gr.HTML("""
        <div style="text-align: center; margin: 20px 0;">
            <h1 style="color: #2196F3; margin-bottom: 10px;">ğŸŒŸ IC Light v2 Complete System</h1>
            <h2 style="color: #4CAF50; margin-bottom: 15px;">Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ IC Light v2</h2>
            <p style="color: #666;">Professional Image Relighting | Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØªØµØ§ÙˆÛŒØ±</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # Input
                gr.HTML("<h3>ğŸ“¤ Input | ÙˆØ±ÙˆØ¯ÛŒ</h3>")
                input_image = gr.Image(type="numpy", label="Upload Image")
                
                # Presets
                gr.HTML("<h3>ğŸ¨ Professional Presets | Ù¾Ø±Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ</h3>")
                preset_dropdown = gr.Dropdown(
                    choices=[(name, prompt) for name, prompt in PRESETS],
                    value=PRESETS[0][1],
                    label="Choose Preset"
                )
                
                # Custom prompt
                prompt_input = gr.Textbox(
                    value=PRESETS[0][1],
                    label="Custom Prompt | Ù¾Ø±Ø§Ù…Ù¾Øª Ø³ÙØ§Ø±Ø´ÛŒ",
                    lines=3
                )
                
                # Light direction
                light_dir = gr.Radio(
                    choices=["Left Light", "Right Light", "Top Light", "Bottom Light"],
                    value="Left Light",
                    label="Light Direction | Ø¬Ù‡Øª Ù†ÙˆØ±"
                )
                
            with gr.Column(scale=1):
                # Output
                gr.HTML("<h3>ğŸ¨ Result | Ù†ØªÛŒØ¬Ù‡</h3>")
                output_image = gr.Image(label="Generated Image")
                
                # Generate button
                generate_btn = gr.Button("ğŸš€ Generate | ØªÙˆÙ„ÛŒØ¯", variant="primary", size="lg")
                
                # Settings
                gr.HTML("<h3>âš™ï¸ Settings | ØªÙ†Ø¸ÛŒÙ…Ø§Øª</h3>")
                with gr.Accordion("Advanced Settings", open=False):
                    width_slider = gr.Slider(256, 1024, 512, step=64, label="Width")
                    height_slider = gr.Slider(256, 1024, 768, step=64, label="Height")
                    steps_slider = gr.Slider(1, 50, 20, step=1, label="Steps")
                    cfg_slider = gr.Slider(1.0, 20.0, 7.0, step=0.5, label="CFG Scale")
                    seed_slider = gr.Slider(-1, 999999999, -1, step=1, label="Seed (-1 for random)")
        
        # Event handlers
        def on_preset_change(preset_value):
            return preset_value
        
        def on_generate(img, prompt, direction, width, height, steps, cfg, seed):
            if img is None:
                return None
            return process_image(img, prompt, direction, int(width), int(height), int(steps), cfg, int(seed))
        
        # Connect events
        preset_dropdown.change(on_preset_change, preset_dropdown, prompt_input)
        
        generate_btn.click(
            on_generate,
            inputs=[input_image, prompt_input, light_dir, width_slider, height_slider, steps_slider, cfg_slider, seed_slider],
            outputs=output_image
        )
        
        # Feature info
        gr.HTML("""
        <div style="margin-top: 30px; text-align: center;">
            <h3>âœ¨ Features | Ø§Ù…Ú©Ø§Ù†Ø§Øª</h3>
            <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 15px;">
                <div style="background: #f0f8ff; padding: 10px; border-radius: 8px;">
                    <strong>ğŸ­ Professional Presets</strong><br/>
                    Ù¾Ø±Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ
                </div>
                <div style="background: #f0fff0; padding: 10px; border-radius: 8px;">
                    <strong>ğŸ’¡ 4-Direction Lighting</strong><br/>
                    Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ Û´ Ø¬Ù‡ØªÙ‡
                </div>
                <div style="background: #fff8dc; padding: 10px; border-radius: 8px;">
                    <strong>ğŸ”§ Background Removal</strong><br/>
                    Ø­Ø°Ù Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡
                </div>
                <div style="background: #ffe4e1; padding: 10px; border-radius: 8px;">
                    <strong>ğŸŒ Bilingual Support</strong><br/>
                    Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¯ÙˆØ²Ø¨Ø§Ù†Ù‡
                </div>
            </div>
        </div>
        """)
    
    return app

if __name__ == "__main__":
    print("ğŸ‰ Starting IC Light v2 Complete System...")
    print("ğŸŒŸ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ IC Light v2...")
    
    app = create_interface()
    app.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        show_api=False
    )
